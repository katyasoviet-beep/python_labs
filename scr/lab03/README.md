# –õ–†3 ‚Äî –¢–µ–∫—Å—Ç—ã –∏ —á–∞—Å—Ç–æ—Ç—ã —Å–ª–æ–≤ (—Å–ª–æ–≤–∞—Ä—å/–º–Ω–æ–∂–µ—Å—Ç–≤–æ)

## –°—Ç—Ä—É–∫—Ç—É—Ä–∞
- –ö–æ–¥: `src/lab03/`
- –°–∫—Ä–∏–Ω—à–æ—Ç—ã: `images/lab03/`

---

## –ó–∞–¥–∞–Ω–∏–µ A ‚Äî `src/lib/text.py`
**–§–∞–π–ª:** `text.py`  
**–†–µ–∞–ª–∏–∑—É–π—Ç–µ —Ñ—É–Ω–∫—Ü–∏–∏:** 
1. `normalize(text: str, *, casefold: bool = True, yo2e: bool = True) -> str`  
   - –ï—Å–ª–∏ `casefold=True` ‚Äî –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ **casefold** (–ª—É—á—à–µ, —á–µ–º `lower` –¥–ª—è –Æ–Ω–∏–∫–æ–¥–∞).  
   - –ï—Å–ª–∏ `yo2e=True` ‚Äî –∑–∞–º–µ–Ω–∏—Ç—å –≤—Å–µ `—ë`/`–Å` –Ω–∞ `–µ`/`–ï`.  
   - –£–±—Ä–∞—Ç—å –Ω–µ–≤–∏–¥–∏–º—ã–µ —É–ø—Ä–∞–≤–ª—è—é—â–∏–µ —Å–∏–º–≤–æ–ª—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, `\t`, `\r`) ‚Üí –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ –ø—Ä–æ–±–µ–ª—ã, —Å—Ö–ª–æ–ø–Ω—É—Ç—å –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –ø—Ä–æ–±–µ–ª—ã –≤ –æ–¥–∏–Ω.
2. `tokenize(text: str) -> list[str]`  
   - –†–∞–∑–±–∏—Ç—å –Ω–∞ ¬´—Å–ª–æ–≤–∞¬ª –ø–æ –Ω–µ–±—É–∫–≤–µ–Ω–Ω–æ-—Ü–∏—Ñ—Ä–æ–≤—ã–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º.  
   - –í –∫–∞—á–µ—Å—Ç–≤–µ —Å–ª–æ–≤–∞ —Å—á–∏—Ç–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–∏–º–≤–æ–ª–æ–≤ `\w` (–±—É–∫–≤—ã/—Ü–∏—Ñ—Ä—ã/–ø–æ–¥—á—ë—Ä–∫–∏–≤–∞–Ω–∏–µ) **–ø–ª—é—Å** –¥–µ—Ñ–∏—Å –≤–Ω—É—Ç—Ä–∏ —Å–ª–æ–≤–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, `–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É`).  
   - –ß–∏—Å–ª–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, `2025`) —Å—á–∏—Ç–∞–µ–º —Å–ª–æ–≤–∞–º–∏.
3. `count_freq(tokens: list[str]) -> dict[str, int]`  
   - –ü–æ–¥—Å—á–∏—Ç–∞—Ç—å —á–∞—Å—Ç–æ—Ç—ã, –≤–µ—Ä–Ω—É—Ç—å —Å–ª–æ–≤–∞—Ä—å `—Å–ª–æ–≤–æ ‚Üí –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ`.
4. `top_n(freq: dict[str, int], n: int = 5) -> list[tuple[str, int]]`  
   - –í–µ—Ä–Ω—É—Ç—å —Ç–æ–ø-N –ø–æ —É–±—ã–≤–∞–Ω–∏—é —á–∞—Å—Ç–æ—Ç—ã; –ø—Ä–∏ —Ä–∞–≤–µ–Ω—Å—Ç–≤–µ ‚Äî –ø–æ –∞–ª—Ñ–∞–≤–∏—Ç—É —Å–ª–æ–≤–∞.

### –¢–µ—Å—Ç-–∫–µ–π—Å—ã:

**normalize**
- `"–ü—Ä–ò–≤–ï—Ç\n–ú–ò—Ä\t"` ‚Üí `"–ø—Ä–∏–≤–µ—Ç –º–∏—Ä"` (casefold + —Å—Ö–ª–æ–ø–Ω—É—Ç—å –ø—Ä–æ–±–µ–ª—ã)  
- `"—ë–∂–∏–∫, –Å–ª–∫–∞"` (`yo2e=True`) ‚Üí `"–µ–∂–∏–∫, –µ–ª–∫–∞"`  
- `"Hello\r\nWorld"` ‚Üí `"hello world"`  
- `"  –¥–≤–æ–π–Ω—ã–µ   –ø—Ä–æ–±–µ–ª—ã  "` ‚Üí `"–¥–≤–æ–π–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã"`

**tokenize** *(–ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ —Ç–µ–∫—Å—Ç —É–∂–µ normalize)*
- `"–ø—Ä–∏–≤–µ—Ç –º–∏—Ä"` ‚Üí `["–ø—Ä–∏–≤–µ—Ç", "–º–∏—Ä"]`  
- `"hello,world!!!"` ‚Üí `["hello", "world"]`  
- `"–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ"` ‚Üí `["–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É", "–∫—Ä—É—Ç–æ"]`  
- `"2025 –≥–æ–¥"` ‚Üí `["2025", "–≥–æ–¥"]`  
- `"emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ"` ‚Üí `["emoji", "–Ω–µ", "—Å–ª–æ–≤–æ"]` (—ç–º–æ–¥–∑–∏ –≤—ã–ø–∞–¥–∞—é—Ç)

**count_freq + top_n**
- –¢–æ–∫–µ–Ω—ã `["a","b","a","c","b","a"]` ‚Üí —á–∞—Å—Ç–æ—Ç—ã `{"a":3,"b":2,"c":1}`;  
  `top_n(..., n=2)` ‚Üí `[("a",3), ("b",2)]`  
- –ü—Ä–∏ —Ä–∞–≤–µ–Ω—Å—Ç–≤–µ —á–∞—Å—Ç–æ—Ç: —Ç–æ–∫–µ–Ω—ã `["bb","aa","bb","aa","cc"]` ‚Üí —á–∞—Å—Ç–æ—Ç—ã `{"aa":2,"bb":2,"cc":1}`;  
  `top_n(..., n=2)` ‚Üí `[("aa",2), ("bb",2)]` (–∞–ª—Ñ–∞–≤–∏—Ç–Ω–∞—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø—Ä–∏ —Ä–∞–≤–µ–Ω—Å—Ç–≤–µ).


### –ö–æ–¥:
```

def normalize(text: str, *, casefold: bool = True, yo2e: bool = True):
    s = text
    if casefold:
        s = s.casefold()
    if yo2e:
        s = s.replace('—ë', '–µ')
    s = s.replace('\t', ' ').replace('\r', ' ').replace('\n', ' ')
    s = ' '.join(s.split())
    return s
def tokenize(text):
    tokens = []
    word = ''
    for ch in text:
        if ch.isalnum() or ch == '_':
            word += ch
        elif ch == '-' and word:
            word += '-'
        else:
            if word:
                tokens.append(word)
                word = ''
    if word:
        tokens.append(word)
    return tokens
def count_freq(tokens):
    freq = {}
    for word in tokens:
        freq[word] = freq.get(word, 0) + 1
    return freq
def top_n(freq, n=5):
    result = sorted(freq.items(), key=lambda pair: (-pair[1], pair[0]))
    return result[:n]

#           –¢–µ—Å—Ç-–∫–µ–π—Å—ã
assert normalize("–ü—Ä–ò–≤–ï—Ç\n–ú–ò—Ä\t") == "–ø—Ä–∏–≤–µ—Ç –º–∏—Ä"
assert normalize("—ë–∂–∏–∫, –Å–ª–∫–∞") == "–µ–∂–∏–∫, –µ–ª–∫–∞"
assert normalize("Hello\r\nWorld") == "hello world"
assert normalize("  –¥–≤–æ–π–Ω—ã–µ   –ø—Ä–æ–±–µ–ª—ã  ") == "–¥–≤–æ–π–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã"

print("normalize —É—Å–ø–µ—à–Ω–æ!")
assert tokenize("–ø—Ä–∏–≤–µ—Ç, –º–∏—Ä!") == ["–ø—Ä–∏–≤–µ—Ç", "–º–∏—Ä"]
assert tokenize("–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ") == ["–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É", "–∫—Ä—É—Ç–æ"]
assert tokenize("2025 –≥–æ–¥") == ["2025", "–≥–æ–¥"]
assert tokenize("hello,world!!!") == ["hello", "world"]
assert tokenize("emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ") == ["emoji", "–Ω–µ", "—Å–ª–æ–≤–æ"]
print("tokenize —É—Å–ø–µ—à–Ω–æ!")
freq = count_freq(["a","b","a","c","b","a"])
assert freq == {"a":3, "b":2, "c":1}
assert top_n(freq, 2) == [("a",3), ("b",2)]
print("count_freq + top_n —É—Å–ø–µ—à–Ω–æ!")

freq2 = count_freq(["bb","aa","bb","aa","cc"])
assert top_n(freq2, 2) == [("aa",2), ("bb",2)]
print("—Ç–∞–π-–±—Ä–µ–π–∫ –ø–æ —Å–ª–æ–≤—É –ø—Ä–∏ —Ä–∞–≤–Ω–æ–π —á–∞—Å—Ç–æ—Ç–µ —É—Å–ø–µ—à–Ω–æ!")

```
**–†–µ–∑—É–ª—å—Ç–∞—Ç:**

![–ó–∞–¥–∞–Ω–∏–µ 1](../../images/lab03/1.png)

---

## –ó–∞–¥–∞–Ω–∏–µ B ‚Äî `src/text_stats.py`
**–§–∞–π–ª:** `text_stats.py`  
–°–∫—Ä–∏–ø—Ç —á–∏—Ç–∞–µ—Ç –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É —Ç–µ–∫—Å—Ç–∞ –∏–∑ **stdin**, –≤—ã–∑—ã–≤–∞–µ—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –∏–∑ `lib/text.py` –∏ –ø–µ—á–∞—Ç–∞–µ—Ç:
1. `–í—Å–µ–≥–æ —Å–ª–æ–≤: <N>`  
2. `–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: <K>`  
3. `–¢–æ–ø-5:` ‚Äî –ø–æ —Å—Ç—Ä–æ–∫–µ –Ω–∞ –∑–∞–ø–∏—Å—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ `—Å–ª–æ–≤–æ:–∫–æ–ª-–≤–æ` (–ø–æ —É–±—ã–≤–∞–Ω–∏—é, –∫–∞–∫ –≤ `top_n`).

### –ü—Ä–∏–º–µ—Ä –∑–∞–ø—É—Å–∫–∞
–í —Ç–µ—Ä–º–∏–Ω–∞–ª–µ:
```
–ü—Ä–∏–≤–µ—Ç, –º–∏—Ä! –ü—Ä–∏–≤–µ—Ç!!!
–í—Å–µ–≥–æ —Å–ª–æ–≤: 3
–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: 2
–¢–æ–ø-5:
–ø—Ä–∏–≤–µ—Ç:2
–º–∏—Ä:1
```

### –ö–æ–¥:
```
from lib.text import normalize, tokenize, count_freq, top_n
text = input()
text = normalize(text)
tokens = tokenize(text)
freq = count_freq(tokens)
top = top_n(freq, 5)
print("–í—Å–µ–≥–æ —Å–ª–æ–≤:", len(tokens))
print("–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤:", len(freq))
print("–¢–æ–ø-5:")
for word, count in top:
    print(f"{word}:{count}")
```

**–°–∫—Ä–∏–Ω—à–æ—Ç:**

![–ó–∞–¥–∞–Ω–∏–µ 2](../../images/lab03/2.png)

---
